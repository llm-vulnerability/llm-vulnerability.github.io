<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NAACL 2024 Tutorial: LLM Vulnerability</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
              <span style="font-size: 80%">NAACL 2024 Tutorial:</span><br />
              LLM Vulnerability
            </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <table>
            <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/.jpeg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/yue_dong.jpeg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/.jpeg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/.jpeg"></td>
              <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/.jpeg"></td>
            </tr>

            <tr>
              <!-- <th scope="row">TR-7</th> -->
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">A</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://yuedong.us/" style="border-radius: 50%">Yue Dong</a><sup>2</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">B</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">C</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">D</a><sup>1</sup>,</td>
            </tr>
            </table>

            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Google,</span>
            <span class="author-block"><sup>2</sup>University of California, Riverside</span>
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <b>Tuesday August 8 10 am - 1 pm (PDT) @ TBA </b>
          </div>
          

          <div class="is-size-5 publication-authors">
            Zoom link available on <a href="" target="_blank">KDD</a>
          </div>
          <div class="is-size-6 publication-authors">
            For those who have not registered to KDD: we will release video recordings after the tutorial
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <!-- QnA: <a href="https://tinyurl.com/retrieval-lm-tutorial" target="_blank"><b></b></a> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this tutorial</h2>
        <div class="content has-text-justified">
          <p>
            Text-editing models have recently become a prominent alternative to seq2seq models for monolingual text-generation tasks such as grammatical error correction, text simplification, and style transfer. These tasks share a common trait – they exhibit a large amount of textual overlap between the source and target texts.
          </p>
          <p>
            Text-editing models take advantage of this observation and learn to generate the output by predicting edit operations applied to the source sequence. In contrast, seq2seq models generate outputs word-by-word from scratch thus making them slow at inference time. Text-editing models provide several benefits over seq2seq models including faster inference speed, higher sample efficiency, and better control and interpretability of the outputs.
          </p>
          <p>
            This tutorial provides a comprehensive overview of the text-edit based models and current state-of-the-art approaches analyzing their pros and cons. We discuss challenges related to deployment and how these models help to mitigate hallucination and bias, both pressing challenges in the field of text generation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Schedule</h2>
        <p>
          Our tutorial will be held on August 8 10 am - 1 pm (PDT).
          <em>Slides may be subject to updates.</em>
        </p>

        <div class="content has-text-justified">

          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg">
          <thead>
            <tr>
              <th class="tg-0pky">Time</th>
              <th class="tg-0lax">Section</th>
              <th class="tg-0lax">Presenter</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-0lax">10:00—10:15</td>
              <td class="tg-0lax">Section 1: Introduction - What are text-editing models? <a href="./slides/1-intro.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Eric</td>
            </tr>
            <tr>
              <td class="tg-0lax">10:15—10:50</td>
              <td class="tg-0lax">Section 2: Model Design  <a href="./slides/2-model.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Eric, Jonathan</td>
            </tr>
            <tr>
              <td class="tg-0lax">10:50-11:25</td>
              <td class="tg-0lax">Section 3: Applications  <a href="./slides/3-application.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Eric, Yue</td>
            </tr>
            </tr>
            <tr>
              <td class="tg-0lax">11:25—11:30</td>
              <td class="tg-0lax">Q & A Session I</td>
              <td class="tg-0lax"></td>
            </tr>
            <!--
            <tr>
              <td class="tg-0lax">11:30—11:45</td>
              <td class="tg-0lax">Coffee break</td>
              <td class="tg-0lax"></td>
            </tr>
            -->
            <tr>
              <td class="tg-0lax">11:30—11:55</td>
              <td class="tg-0lax">Section 4: Controllable Generation<a href="./slides/4-control-gen.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Yue</td>
            </tr>
            <tr>
              <td class="tg-0lax">11:55—12:10</td>
              <td class="tg-0lax">Section 5: Multilingual Text Editing <a href="./slides/5-multimodal.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Yue</td>
            </tr>
            <tr>
              <td class="tg-0lax">12:10—12:50</td>
              <td class="tg-0lax">Section 6: Faster (Large) Language Models  <a href="./slides/6-production.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Jonathan</td>
            </tr>
            <tr>
              <td class="tg-0lax">12:50—12:55</td>
              <td class="tg-0lax">Section 7: Recommendations & Future Directions <a href="./slides/7-conclusion.pdf" target='_blank'>[Slides]</a> <a href="./slides/references.pdf" target='_blank'>[References]</a></td>
              <td class="tg-0lax">Eric</td>
            </tr>
            <tr>
              <td class="tg-0lax">12:55—13:00</td>
              <td class="tg-0lax">Q & A Session II</td>
              <td class="tg-0lax"></td>
            </tr>
          </tbody>
          </table>
        </div>
      </div>
    </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reading List</h2>

        <!--
        <p><b>Bold papers</b> are discussed in detail during our tutorial.</p>

        <br />
        
        
        <h3 class="title is-5">Section 3: Architecture</h3>

        <ul>
          <li><a href="https://arxiv.org/abs/2002.08909"><b>REALM: Retrieval-Augmented Language Model Pre-Training</b></a> (Guu et al., 2020)</li>
          <li><a href="https://arxiv.org/pdf/2302.00083.pdf"><b>In-Context Retrieval-Augmented Language Models</b></a> (Ram et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2301.12652.pdf"><b>REPLUG: Retrieval-Augmented Black-Box Language Models</b></a> (Shi et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2112.04426.pdf"><b>Improving language models by retrieving from trillions of tokens</b></a> (Borgeaud et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/1911.00172.pdf"><b>Generalization through Memorization: Nearest Neighbor Language Models</b></a> (Khandelwal et al., 2020)</li>
          <li><a href="https://arxiv.org/abs/2305.06983">Active Retrieval Augmented Generation</a> (Jiang et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2109.04212">Efficient Nearest Neighbor Language Models</a> (He et al., 2021)</li>
          <li><a href="https://arxiv.org/abs/2210.15859">You can't pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM</a> (Drozdov et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2201.12431">Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval</a> (Alon et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2004.07202">Entities as Experts: Sparse Memory Access with Entity Supervision</a> (Févry et al., 2020)</li>
          <li><a href="https://arxiv.org/abs/2110.06176">Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</a> (de Jong et al., 2021)</li>
          <li><a href="https://arxiv.org/abs/2203.08913">Memorizing Transformers</a> (Wu et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2305.01625">Unlimiformer: Long-Range Transformers with Unlimited Length Input</a> (Bertsch et al. 2023)</li>
          <li><a href="https://arxiv.org/abs/2306.13421">Long-range Language Modeling with Self-retrieval</a> (Rubin & Brent, 2023)</li>
        </ul>
        
        <br />

        <h3 class="title is-5">Section 4: Training</h3>

        <ul>
          <li><a href="https://arxiv.org/abs/2004.04906"><b>Dense Passage Retrieval for Open-Domain Question Answering</b></a> (Karpukhin et al., 2020)</li>
          <li><a href="https://arxiv.org/abs/2112.04426"><b>Improving language models by retrieving from trillions of tokens</b></a> (Borgeaud et al., 2022 ;also in Section 3)</li>
          <li><a href="https://arxiv.org/abs/2208.03299"><b>Atlas: Few-shot Learning with Retrieval Augmented Language Models</b></a> (Izacard et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2205.12674"><b>Training Language Models with Memory Augmentation</b></a> (Zhong et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2112.09118.pdf">Unsupervised Dense Information Retrieval with Contrastive Learning</a> (Izacard et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2302.00083.pdf">In-Context Retrieval-Augmented Language Models</a> (Ram et al., 2023; also in Section 3)</li>
          <li><a href="https://arxiv.org/pdf/2301.12652.pdf">REPLUG: Retrieval-Augmented Black-Box Language Models</a> (Shi et al., 2023; also in Section 3)</li>
          <li><a href="https://arxiv.org/abs/2002.08909">REALM: Retrieval-Augmented Language Model Pre-Training</a> (Guu et al., 2020; also in Section 3)</li>
          <li><a href="https://arxiv.org/abs/2212.01349">Nonparametric Masked Language Modeling</a> (Min et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2306.13421">Long-range Language Modeling with Self-retrieval</a> (Rubin et al., 2023)</li>
        </ul>

        <br />

        <h3 class="title is-5">Section 5: Application</h3>

        <ul>
          <li><a href="https://arxiv.org/abs/2208.03299"><b>Atlas: Few-shot Learning with Retrieval Augmented Language Models</b></a> (Izacard et al., 2022; also in Section 4)</li>
          <li><a href="https://arxiv.org/abs/2203.11147"><b>Teaching language models to support answers with verified quotes</b></a> (Menick et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2301.12652.pdf"><b>REPLUG: Retrieval-Augmented Black-Box Language Models</b></a> (Shi et al., 2023; also in Section 3)</li>

        </ul>

        <br />

        <h3 class="title is-5">Section 6: Extension</h3>
        
        <ul>
          <li><a href="https://arxiv.org/abs/2107.11976">One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval</a> (Asai et al., 2021)</li>
          <li><a href="https://arxiv.org/abs/2211.12561">Retrieval-Augmented Multimodal Language Modeling</a> (Yasunaga et al., 2023)</li>
        </ul>

        
        <br />
        <h3 class="title is-5">Section 7: Challenges & Opportunities</h3>
        <ul>
          <li><a href="https://arxiv.org/abs/2305.14625">KNN-LM Does Not Improve Open-ended Text Generation</a> (Wang et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2212.09146">Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model</a> (BehnamGhader et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2212.14024">Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</a> (Khattab et al., 2022)</li>
        </ul>
        -->
        <h3 class="title is-5">Prerequisites</h3>
         <ul>
            <li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>, Vaswani et al., 2017.</li>
            <li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Devlin et al., 2018.</li>
         </ul>

         <br />
         <h3 class="title is-5">Text-Editing Methods Discussed during the Tutorial</h3>
         <ul>
            <li><a href="https://arxiv.org/abs/2205.12209">EdiT5</a>, Mallinson et al., 2022.</li>
            <li><a href="https://aclanthology.org/P19-1331/">EditNTS</a>, Dong et al., 2019.</li>
            <li><a href="https://aclanthology.org/2020.findings-emnlp.111/">Felix</a>, Mallinson et al., 2020.</li>
            <li><a href="https://aclanthology.org/2020.bea-1.16/">GECToR</a>, Omelianchuk et al., 2020.</li>
            <li><a href="https://www.aaai.org/AAAI22Papers/AAAI-3890.JinL.pdf">HCT</a>, Jin et al., 2022.</li>
            <li><a href="https://aclanthology.org/D19-1510/">LaserTagger</a>, Malmi et al., 2019.</li>
            <li><a href="https://proceedings.neurips.cc/paper/2019/file/675f9820626f5bc0afb47b57890b466e-Paper.pdf">LevT</a>, Gu et al., 2019.</li>
            <li><a href="https://aclanthology.org/2021.findings-acl.344/">LEWIS</a>, Reid and Zhong, 2021.</li>
            <li><a href="https://aclanthology.org/2020.emnlp-main.699/">Masker</a>, Malmi et al., 2020.</li>
            <li><a href="https://aclanthology.org/D19-1435/">PIE</a>, Awasthi et al., 2019.</li>
            <li><a href="https://arxiv.org/abs/2009.13166">RUN</a>, Liu et al., 2020.</li>
            <li><a href="https://aclanthology.org/2020.emnlp-main.418/">Seq2Edits</a>, Stahlberg and Kumar, 2020.</li>
            <li><a href="https://aclanthology.org/I17-1030/">SL</a>, Alva-Machego et al., 2017.</li>
         </ul>

        <br />
        <h3 class="title is-5">Acknowledgements</h3>
        <ul>
        We would like to thank ABC.
        </ul>

      </div>


    </div>
</section>

<!--
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kdd2023-text-editing-tutorial,
  author    = {X},
  title     = { NAACL 2024 Tutorial: LLM Vunerability},
  journal   = { NAACL 2024 },
  year      = { 2024 },
}</code></pre>
  </div>
</section>
-->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/kdd2023-text-editing" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
