<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>KDD 2023 Tutorial: Fast Text Generation with Text-Editing Models</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
              <span style="font-size: 80%">KDD 2023 Tutorial:</span><br />
              Fast Text Generation with Text-Editing Models
            </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <table>
            <tr>
                <!-- <th scope="row">TR-7</th> -->
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/eric_malmi.jpeg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/yue_dong.jpeg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/jonathan_mallinson.jpeg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/aleksandr_chuklin.jpeg"></td>
              <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/jakub_adamek.jpeg"></td>
            </tr>

            <tr>
              <!-- <th scope="row">TR-7</th> -->
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Eric Malmi</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://yuedong.us/" style="border-radius: 50%">Yue Dong</a><sup>2</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Jonathan Mallinson</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Aleksandr Chuklin</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Jakub Adamek</a><sup>1</sup>,</td>
            </tr>

               <tr>
              <!-- <th scope="row">TR-7</th> -->
              <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/daniil_mirylenka.jpeg"></td>
              <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/felix_stahlberg.jpeg"></td>
              <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/sebastian_krause.jpeg"></td>
         <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/shankar_kumar.jpeg"></td>
                <td width="20%" style="text-align: center; padding: 3px"><img width="150px" height="150px" src="static/imgs/aliaksei_severyn.jpeg"></td>
          </tr>
            <tr>
              <!-- <th scope="row">TR-7</th> -->

              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Daniil Mirylenka</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Felix Stahlberg</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Sebastian Krause</a><sup>1</sup>,</td>
              <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Shankar Kumar</a><sup>1</sup>,</td>
                <td width="20%" style="text-align: center"><a href="https://" style="border-radius: 50%">Aliaksei Severyn</a><sup>1</sup>,</td>
            </tr>
            </table>

            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Google,</span>
            <span class="author-block"><sup>2</sup>University of California, Riverside</span>
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <b>Tuesday August 8 10 am - 1 pm (PDT) @ TBA </b>
          </div>
          

          <div class="is-size-5 publication-authors">
            Zoom link available on <a href="" target="_blank">KDD</a>
          </div>
          <div class="is-size-6 publication-authors">
            For those who have not registered to KDD: we will release video recordings after the tutorial
          </div>
          <br />
          <div class="is-size-5 publication-authors">
            <!-- QnA: <a href="https://tinyurl.com/retrieval-lm-tutorial" target="_blank"><b></b></a> -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this tutorial</h2>
        <div class="content has-text-justified">
          <p>
            Text-editing models have recently become a prominent alternative to seq2seq models for monolingual text-generation tasks such as grammatical error correction, text simplification, and style transfer. These tasks share a common trait – they exhibit a large amount of textual overlap between the source and target texts.
          </p>
          <p>
            Text-editing models take advantage of this observation and learn to generate the output by predicting edit operations applied to the source sequence. In contrast, seq2seq models generate outputs word-by-word from scratch thus making them slow at inference time. Text-editing models provide several benefits over seq2seq models including faster inference speed, higher sample efficiency, and better control and interpretability of the outputs.
          </p>
          <p>
            This tutorial provides a comprehensive overview of the text-edit based models and current state-of-the-art approaches analyzing their pros and cons. We discuss challenges related to deployment and how these models help to mitigate hallucination and bias, both pressing challenges in the field of text generation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Schedule</h2>
        <p>
          Our tutorial will be held on August 8 10 am - 1 pm (PDT).
          <em>Slides may be subject to updates.</em>
        </p>

        <div class="content has-text-justified">

          <style type="text/css">
          .tg  {border-collapse:collapse;border-spacing:0;}
          .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
            font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
          .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
          .tg .tg-0lax{text-align:left;vertical-align:top}
          </style>
          <table class="tg">
          <thead>
            <tr>
              <th class="tg-0pky">Time</th>
              <th class="tg-0lax">Section</th>
              <th class="tg-0lax">Presenter</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="tg-0lax">10:00—10:15</td>
              <td class="tg-0lax">Section 1: Introduction - What are text-editing models? <a href="./slides/1-intro.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Eric</td>
            </tr>
            <tr>
              <td class="tg-0lax">10:15—10:50</td>
              <td class="tg-0lax">Section 2: Model Design  <a href="./slides/2-model.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Eric, Jonathan</td>
            </tr>
            <tr>
              <td class="tg-0lax">10:50-11:25</td>
              <td class="tg-0lax">Section 3: Applications  <a href="./slides/3-application.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Eric, Yue</td>
            </tr>
            </tr>
            <tr>
              <td class="tg-0lax">11:25—11:30</td>
              <td class="tg-0lax">Q & A Session I</td>
              <td class="tg-0lax"></td>
            </tr>
            <!--
            <tr>
              <td class="tg-0lax">11:30—11:45</td>
              <td class="tg-0lax">Coffee break</td>
              <td class="tg-0lax"></td>
            </tr>
            -->
            <tr>
              <td class="tg-0lax">11:30—11:55</td>
              <td class="tg-0lax">Section 4: Controllable Generation<a href="./slides/4-control-gen.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Yue</td>
            </tr>
            <tr>
              <td class="tg-0lax">11:55—12:10</td>
              <td class="tg-0lax">Section 5: Multilingual Text Editing <a href="./slides/5-multimodal.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Yue</td>
            </tr>
            <tr>
              <td class="tg-0lax">12:10—12:50</td>
              <td class="tg-0lax">Section 6: Faster (Large) Language Models  <a href="./slides/6-production.pdf" target='_blank'>[Slides]</a></td>
              <td class="tg-0lax">Jonathan</td>
            </tr>
            <tr>
              <td class="tg-0lax">12:50—12:55</td>
              <td class="tg-0lax">Section 7: Recommendations & Future Directions <a href="./slides/7-conclusion.pdf" target='_blank'>[Slides]</a> <a href="./slides/references.pdf" target='_blank'>[References]</a></td>
              <td class="tg-0lax">Eric</td>
            </tr>
            <tr>
              <td class="tg-0lax">12:55—13:00</td>
              <td class="tg-0lax">Q & A Session II</td>
              <td class="tg-0lax"></td>
            </tr>
          </tbody>
          </table>
        </div>
      </div>
    </div>

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reading List</h2>

        <p><b>Bold papers</b> are discussed in detail during our tutorial.</p>

        <br />
        
        
        <h3 class="title is-5">Section 3: Architecture</h3>

        <ul>
          <li><a href="https://arxiv.org/abs/2002.08909"><b>REALM: Retrieval-Augmented Language Model Pre-Training</b></a> (Guu et al., 2020)</li>
          <li><a href="https://arxiv.org/pdf/2302.00083.pdf"><b>In-Context Retrieval-Augmented Language Models</b></a> (Ram et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2301.12652.pdf"><b>REPLUG: Retrieval-Augmented Black-Box Language Models</b></a> (Shi et al., 2023)</li>
          <li><a href="https://arxiv.org/pdf/2112.04426.pdf"><b>Improving language models by retrieving from trillions of tokens</b></a> (Borgeaud et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/1911.00172.pdf"><b>Generalization through Memorization: Nearest Neighbor Language Models</b></a> (Khandelwal et al., 2020)</li>
          <li><a href="https://arxiv.org/abs/2305.06983">Active Retrieval Augmented Generation</a> (Jiang et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2109.04212">Efficient Nearest Neighbor Language Models</a> (He et al., 2021)</li>
          <li><a href="https://arxiv.org/abs/2210.15859">You can't pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM</a> (Drozdov et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2201.12431">Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval</a> (Alon et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2004.07202">Entities as Experts: Sparse Memory Access with Entity Supervision</a> (Févry et al., 2020)</li>
          <li><a href="https://arxiv.org/abs/2110.06176">Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</a> (de Jong et al., 2021)</li>
          <li><a href="https://arxiv.org/abs/2203.08913">Memorizing Transformers</a> (Wu et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2305.01625">Unlimiformer: Long-Range Transformers with Unlimited Length Input</a> (Bertsch et al. 2023)</li>
          <li><a href="https://arxiv.org/abs/2306.13421">Long-range Language Modeling with Self-retrieval</a> (Rubin & Brent, 2023)</li>
        </ul>
        
        <br />

        <h3 class="title is-5">Section 4: Training</h3>

        <ul>
          <li><a href="https://arxiv.org/abs/2004.04906"><b>Dense Passage Retrieval for Open-Domain Question Answering</b></a> (Karpukhin et al., 2020)</li>
          <li><a href="https://arxiv.org/abs/2112.04426"><b>Improving language models by retrieving from trillions of tokens</b></a> (Borgeaud et al., 2022 ;also in Section 3)</li>
          <li><a href="https://arxiv.org/abs/2208.03299"><b>Atlas: Few-shot Learning with Retrieval Augmented Language Models</b></a> (Izacard et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2205.12674"><b>Training Language Models with Memory Augmentation</b></a> (Zhong et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2112.09118.pdf">Unsupervised Dense Information Retrieval with Contrastive Learning</a> (Izacard et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2302.00083.pdf">In-Context Retrieval-Augmented Language Models</a> (Ram et al., 2023; also in Section 3)</li>
          <li><a href="https://arxiv.org/pdf/2301.12652.pdf">REPLUG: Retrieval-Augmented Black-Box Language Models</a> (Shi et al., 2023; also in Section 3)</li>
          <li><a href="https://arxiv.org/abs/2002.08909">REALM: Retrieval-Augmented Language Model Pre-Training</a> (Guu et al., 2020; also in Section 3)</li>
          <li><a href="https://arxiv.org/abs/2212.01349">Nonparametric Masked Language Modeling</a> (Min et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2306.13421">Long-range Language Modeling with Self-retrieval</a> (Rubin et al., 2023)</li>
        </ul>

        <br />

        <h3 class="title is-5">Section 5: Application</h3>

        <ul>
          <li><a href="https://arxiv.org/abs/2208.03299"><b>Atlas: Few-shot Learning with Retrieval Augmented Language Models</b></a> (Izacard et al., 2022; also in Section 4)</li>
          <li><a href="https://arxiv.org/abs/2203.11147"><b>Teaching language models to support answers with verified quotes</b></a> (Menick et al., 2022)</li>
          <li><a href="https://arxiv.org/pdf/2301.12652.pdf"><b>REPLUG: Retrieval-Augmented Black-Box Language Models</b></a> (Shi et al., 2023; also in Section 3)</li>
          <!-- <li><a href="https://arxiv.org/abs/2212.14024"><b>Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</b></a></li> -->
          <!-- <li><a href="https://arxiv.org/abs/2305.06311">Automatic evaluations of Attributions of large language models</a></li> -->
          <!-- <li><a href="https://arxiv.org/abs/2210.08726">RARR: Researching and Revising What Language Models Say, Using Language Models</a></li> -->
          <!-- <li><a href="https://arxiv.org/abs/2305.14625">kNN-LM Does Not Improve Open-ended Text Generation</a></li> -->
          <!-- <li><a href="https://arxiv.org/abs/2212.09146">Can Retriever-augmented Language Models Reason? The Blame Game between the Retriever and the Language Model</a></li> -->
          <li><a href="https://arxiv.org/abs/2205.13792"><b>kNN-Prompt: Nearest Neighbor Zero-Shot Inference</b></a> (Shi et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2212.08037"><b>Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models</b></a> (Bohnet et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2207.05987"><b>DocPrompting: Generating Code by Retrieving the Docs</b></a> (Zhou et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2212.10511">When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories</a> (Mallen et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2212.01349">Nonparametric Masked Language Modeling</a> (Min et al., 2023; also in Section 4)</li>
          <!-- <li><a href="https://arxiv.org/abs/2305.14627">Enabling Large Language Models to Generate Text with Citations</a> (Gao et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2304.09848">Evaluating Verifiability in Generative Search Engines</a> (Liu et al., 2023)</li> -->
          <li><a href="https://arxiv.org/abs/2305.14251">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a> (Min et al., 2023)</li>
          <!-- <li><a href="https://arxiv.org/abs/2305.14888">Privacy Implications of Retrieval-Based Language Models</a></li> -->
        </ul>

        <br />

        <h3 class="title is-5">Section 6: Extension</h3>
        
        <ul>
          <li><a href="https://arxiv.org/abs/2107.11976">One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval</a> (Asai et al., 2021)</li>
          <li><a href="https://arxiv.org/abs/2211.12561">Retrieval-Augmented Multimodal Language Modeling</a> (Yasunaga et al., 2023)</li>
        </ul>
        <!--<ul>
          <li><a href="https://arxiv.org/abs/2107.11976">One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval</a></li>
          <li><a href="https://arxiv.org/abs/2211.12561">Retrieval-Augmented Multimodal Language Modeling</a></li>
        </ul> -->

        
        <br />
        <h3 class="title is-5">Section 7: Challenges & Opportunities</h3>
        <ul>
          <li><a href="https://arxiv.org/abs/2305.14625">KNN-LM Does Not Improve Open-ended Text Generation</a> (Wang et al., 2023)</li>
          <li><a href="https://arxiv.org/abs/2212.09146">Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model</a> (BehnamGhader et al., 2022)</li>
          <li><a href="https://arxiv.org/abs/2212.14024">Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP</a> (Khattab et al., 2022)</li>
        </ul>
      </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kdd2023-text-editing-tutorial,
  author    = { AEric Malmi, Yue Dong, Jonathan Mallinson, Aleksandr Chuklin, Jakub Adamek,
      Daniil Mirylenka,	Felix Stahlberg, Sebastian Krause, Shankar Kumar, Aliaksei Severyn},
  title     = { KDD 2023 Tutorial: Fast Text Generation with Text-Editing Models },
  journal   = { KDD 2023 },
  year      = { 2023 },
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/kdd2023-text-editing" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
